🧠 Enhancing LLMs with RAG and Efficient Fine-Tuning Techniques

This project demonstrates how Retrieval-Augmented Generation (RAG) combined with efficient fine-tuning techniques like LoRA and qLoRA can significantly enhance the performance of Large Language Models (LLMs) while reducing resource costs. It evaluates the impact of these techniques across models such as Mistral-7B, GEMA, and LLaMA2.

⸻

🎯 Objective

To explore and compare how integrating RAG and applying parameter-efficient fine-tuning (PEFT) methods improves the contextual performance, accuracy, and cost-efficiency of state-of-the-art LLMs.

🚀 Features
	•	📚 RAG Integration: Combined document retrieval with LLM generation to provide contextually grounded answers.
	•	⚙️ LoRA/qLoRA Fine-Tuning: Applied lightweight fine-tuning strategies to adapt LLMs efficiently on domain-specific tasks.
	•	📊 Multi-Model Benchmarking: Evaluated performance across Mistral-7B, GEMA, and LLaMA2.
	•	💡 Cost & Performance Optimization: Demonstrated lower GPU/memory usage and improved inference quality using quantization.
